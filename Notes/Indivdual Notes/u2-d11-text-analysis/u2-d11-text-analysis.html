<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Text analysis   📃</title>
    <meta charset="utf-8" />
    <script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
    <link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding-0.20/datatables.js"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
    <script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="../xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Text analysis <br> 📃

---


## Packages

In addition to `tidyverse` we will be using four other packages today


```r
library(tidyverse)
library(tidytext)
library(genius)
library(wordcloud)
library(DT)
library(stopwords)
conflicted::conflict_prefer("filter","dplyr")
```

---

## Tidytext

- Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.
- Learn more at https://www.tidytextmining.com/.

---

## What is tidy text?

*There Is a Light That Never Goes Out* by The Smiths 


```r
text &lt;- c("Take me out tonight",
          "Where there's music and there's people",
          "And they're young and alive",
          "Driving in your car",
          "I never never want to go home",
          "Because I haven't got one",
          "Anymore")

text
```

```
## [1] "Take me out tonight"                   
## [2] "Where there's music and there's people"
## [3] "And they're young and alive"           
## [4] "Driving in your car"                   
## [5] "I never never want to go home"         
## [6] "Because I haven't got one"             
## [7] "Anymore"
```

---

## What is tidy text?


```r
text_df &lt;- tibble(line = 1:7, text = text)

text_df
```

```
## # A tibble: 7 x 2
##    line text                                  
##   &lt;int&gt; &lt;chr&gt;                                 
## 1     1 Take me out tonight                   
## 2     2 Where there's music and there's people
## 3     3 And they're young and alive           
## 4     4 Driving in your car                   
## 5     5 I never never want to go home         
## 6     6 Because I haven't got one             
## 7     7 Anymore
```

---

## What is tidy text?


```r
text_df %&gt;%
  unnest_tokens(word, text)
```

```
## # A tibble: 32 x 2
##     line word   
##    &lt;int&gt; &lt;chr&gt;  
##  1     1 take   
##  2     1 me     
##  3     1 out    
##  4     1 tonight
##  5     2 where  
##  6     2 there's
##  7     2 music  
##  8     2 and    
##  9     2 there's
## 10     2 people 
## # ... with 22 more rows
```

---

class: middle

# What are you listening to?

---

## From class survey(s)

&gt; What is your favorite song? (excluding those with derogatory words)

.midi[

```r
listening &lt;- read_csv("data/listening.csv")
listening
```

```
## # A tibble: 16 x 1
##    songs                                            
##    &lt;chr&gt;                                            
##  1 "Enchanted -Taylor Swift"                        
##  2 "Tonight Tonight by Hot Chella Rae"              
##  3 "Street Lights by Kanye West"                    
##  4 "Video Killed the Radio Star\x97the Buggles"     
##  5 "Counting stars"                                 
##  6 "A Whiter Shade of Pale-Procol Harum"            
##  7 "Ocean Man by Ween"                              
##  8 "1, 2 Many by Luke Combs"                        
##  9 "All Too Well (10 minute version) - Taylor Swift"
## 10 "Best of Both Worlds - Hannah Montana"           
## 11 "Time of Your Life - Green Day"                  
## 12 "Landslide - Fleetwood Mac"                      
## 13 "anything Dolly Parton sings"                    
## 14 "anything Lee Brice sings"                       
## 15 "Pretty\xa0Fly for a White Guy\xa0"              
## 16 "U2 Anthem"
```
]

---

## Looking for commonalities

.midi[

```r
listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 67 x 2
##    word         n
##    &lt;chr&gt;    &lt;int&gt;
##  1 by           4
##  2 of           3
##  3 a            2
##  4 anything     2
##  5 sings        2
##  6 swift        2
##  7 taylor       2
##  8 the          2
##  9 tonight      2
## 10 1            1
## # ... with 57 more rows
```
]

---

## Stop words

- In computing, stop words are words which are filtered out before or after processing of natural language data (text).
- They usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.

---

## English stop words


```r
get_stopwords()
```

```
## # A tibble: 175 x 2
##    word      lexicon 
##    &lt;chr&gt;     &lt;chr&gt;   
##  1 i         snowball
##  2 me        snowball
##  3 my        snowball
##  4 myself    snowball
##  5 we        snowball
##  6 our       snowball
##  7 ours      snowball
##  8 ourselves snowball
##  9 you       snowball
## 10 your      snowball
## # ... with 165 more rows
```

---

## Spanish stop words


```r
get_stopwords(language = "es")
```

```
## # A tibble: 308 x 2
##    word  lexicon 
##    &lt;chr&gt; &lt;chr&gt;   
##  1 de    snowball
##  2 la    snowball
##  3 que   snowball
##  4 el    snowball
##  5 en    snowball
##  6 y     snowball
##  7 a     snowball
##  8 los   snowball
##  9 del   snowball
## 10 se    snowball
## # ... with 298 more rows
```

---

## Various lexicons

See `?get_stopwords` for more info.

.midi[

```r
get_stopwords(source = "smart")
```

```
## # A tibble: 571 x 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           smart  
##  2 a's         smart  
##  3 able        smart  
##  4 about       smart  
##  5 above       smart  
##  6 according   smart  
##  7 accordingly smart  
##  8 across      smart  
##  9 actually    smart  
## 10 after       smart  
## # ... with 561 more rows
```
]

---

## Back to: Looking for commonalities


```r
listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
* anti_join(stop_words) %&gt;%
* dplyr::filter(!(word %in% c("1", "2", "3", "4", "5"))) %&gt;%
  count(word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 51 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 sings       2
##  2 swift       2
##  3 taylor      2
##  4 tonight     2
##  5 10          1
##  6 anthem      1
##  7 brice       1
##  8 buggles     1
##  9 chella      1
## 10 combs       1
## # ... with 41 more rows
```

---

## Top 20 common words in songs

.pull-left[
.small[

```r
top20_songs &lt;- listening %&gt;%
  unnest_tokens(word, songs) %&gt;%
  anti_join(stop_words) %&gt;%
  dplyr::filter(
    !(word %in% c("1", "2", "3", "4", "5"))
    ) %&gt;%
  count(word) %&gt;%
  top_n(20)
```
]
]
.pull-right[
.midi[

```r
top20_songs %&gt;%
  arrange(desc(n))
```

```
## # A tibble: 51 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 sings       2
##  2 swift       2
##  3 taylor      2
##  4 tonight     2
##  5 10          1
##  6 anthem      1
##  7 brice       1
##  8 buggles     1
##  9 chella      1
## 10 combs       1
## # ... with 41 more rows
```
]
]
---

## Visualizing commonalities: bar chart

.midi[
![](u2-d11-text-analysis_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]

---

... the code


```r
ggplot(top20_songs, aes(x = fct_reorder(word, n), y = n)) +
  geom_col() +
  labs(x = "Common words", y = "Count") +
  coord_flip()
```


---

## Visualizing commonalities: wordcloud

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-15-1.png" width="70%" /&gt;

---

... and the code


```r
set.seed(1234)
wordcloud(words = top20_songs$word, 
          freq = top20_songs$n, 
          colors = brewer.pal(5,"Blues"),
          random.order = FALSE)
```

---

class: middle

# Analyzing lyrics of one artist
## Let's get more data

We'll use the **genius** package to get song data from [Genius](https://genius.com/).

- `genius_album()`: download artists, and track names an entire album
- `add_genius()`: download for multiple albums
- The author of this package recently decided to end support for it for pulling lyrics. It relied on web-scraping and he did not want to support any misuse of the artists lyrics they own.
- This is a particularly alarming problem when someone starts to use your tools on a massive scale. 
- He may have intended for people to do what we are doing - pulling in a few lyrics. 
- The result was likely companies using his tool to do large scale data analysis projects about artists and their lyrics (likely for marketing)

---

## Singers most recent-ish albums
Taylors most recent album. 

```r
artist_albums &lt;- tribble(
  ~artist,      ~album,
  "Taylor Swift", "Red (Taylor's Version)")

artist &lt;- artist_albums %&gt;%
  add_genius(artist, album, "album")

# attach the lyrics (the genius functions no longer supported)
artist_lyrics &lt;- read_csv('data/artistlyrics.csv')
artist &lt;- artist %&gt;%
  mutate(track_title = str_squish(track_title))%&gt;%
  left_join(artist_lyrics,by="track_title") %&gt;%
  mutate(lyric = str_replace_all(lyric,"\\\n|\\[.*\\]"," "),
         lyric = str_squish(lyric))
```

---

## Songs in the four albums

.small[
<div id="htmlwidget-aa12f815ad271cf06cb8" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-aa12f815ad271cf06cb8">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31"],["Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)","Red (Taylor's Version)"],["State of Grace (Taylor's Version)","Red (Taylor's Version)","Treacherous (Taylor's Version)","I Knew You Were Trouble (Taylor's Version)","All Too Well (Taylor's Version)","22 (Taylor's Version)","I Almost Do (Taylor's Version)","We Are Never Ever Getting Back Together (Taylor's Version)","Stay Stay Stay (Taylor's Version)","The Last Time (Taylor's Version) (Ft. Gary Lightbody)","Holy Ground (Taylor's Version)","Sad Beautiful Tragic (Taylor's Version)","The Lucky One (Taylor's Version)","Everything Has Changed (Taylor's Version) (Ft. Ed Sheeran)","Starlight (Taylor's Version)","Begin Again (Taylor's Version)","The Moment I Knew (Taylor's Version)","Come Back...Be Here (Taylor's Version)","Girl At Home (Taylor's Version)","State Of Grace (Acoustic Version) [Taylor's Version]","Ronan (Taylor's Version)","Better Man (Taylor's Version) [From the Vault]","Nothing New (Taylor's Version) [From the Vault] (Ft. Phoebe Bridgers)","Babe (Taylor's Version) [From the Vault]","Message In A Bottle (Taylor's Version) [From The Vault]","I Bet You Think About Me (Taylor's Version) [From the Vault] (Ft. Chris Stapleton)","Forever Winter (Taylor's Version) [From the Vault]","Run (Taylor's Version) [From the Vault] (Ft. Ed Sheeran)","The Very First Night (Taylor's Version) [From the Vault]","All Too Well (10 Minute Version) (Taylor's Version) [From the Vault]","A Message From Taylor"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>album<\/th>\n      <th>track_title<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"p","order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
]

---

## Tidy up your lyrics!


```r
artist_lyrics &lt;- artist %&gt;%
  unnest_tokens(word, lyric)

artist_lyrics
```

```
## # A tibble: 11,551 x 5
##    artist       album                  track_n track_title                word  
##    &lt;chr&gt;        &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt; 
##  1 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ i'm   
##  2 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ walkin
##  3 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ fast  
##  4 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ throu~
##  5 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ the   
##  6 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ traff~
##  7 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ lights
##  8 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ busy  
##  9 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ stree~
## 10 Taylor Swift Red (Taylor's Version)       1 State of Grace (Taylor's ~ and   
## # ... with 11,541 more rows
```

---

## What are the most common words?


```r
artist_lyrics %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 1,292 x 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 you     606
##  2 i       495
##  3 and     390
##  4 the     360
##  5 oh      251
##  6 to      190
##  7 a       184
##  8 in      183
##  9 it      181
## 10 me      174
## # ... with 1,282 more rows
```

---

## Remove Stop Words

.midi[

```r
artist_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 955 x 2
##    word         n
##    &lt;chr&gt;    &lt;int&gt;
##  1 time       100
##  2 ooh         76
##  3 run         57
##  4 love        54
##  5 red         53
##  6 yeah        47
##  7 night       40
##  8 remember    39
##  9 trouble     34
## 10 stay        33
## # ... with 945 more rows
```
]

---

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-22-1.png" width="60%" /&gt;

---

... and the code


```r
artist_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  count(word)%&gt;%
  top_n(20) %&gt;%
  ggplot(aes(fct_reorder(word, n), n)) +
    geom_col() +
    labs(title = "Frequency of Artists lyrics",
         y = "",
         x = "") +
    coord_flip()
```

---

class: middle

# Sentiment analysis

---

## Sentiment analysis

- One way to analyze the sentiment of a text is to consider the text as a combination of its individual words 
- and the sentiment content of the whole text as the sum of the sentiment content of the individual words

---

## Sentiment lexicons

.pull-left[

```r
get_sentiments("afinn")
```

```
## # A tibble: 2,477 x 2
##    word       value
##    &lt;chr&gt;      &lt;dbl&gt;
##  1 abandon       -2
##  2 abandoned     -2
##  3 abandons      -2
##  4 abducted      -2
##  5 abduction     -2
##  6 abductions    -2
##  7 abhor         -3
##  8 abhorred      -3
##  9 abhorrent     -3
## 10 abhors        -3
## # ... with 2,467 more rows
```
]
.pull-right[

```r
get_sentiments("bing") 
```

```
## # A tibble: 6,786 x 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 2-faces     negative 
##  2 abnormal    negative 
##  3 abolish     negative 
##  4 abominable  negative 
##  5 abominably  negative 
##  6 abominate   negative 
##  7 abomination negative 
##  8 abort       negative 
##  9 aborted     negative 
## 10 aborts      negative 
## # ... with 6,776 more rows
```
]

---

## Sentiment lexicons

.pull-left[

```r
get_sentiments("nrc")
```

```
## # A tibble: 13,875 x 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 abacus      trust    
##  2 abandon     fear     
##  3 abandon     negative 
##  4 abandon     sadness  
##  5 abandoned   anger    
##  6 abandoned   fear     
##  7 abandoned   negative 
##  8 abandoned   sadness  
##  9 abandonment anger    
## 10 abandonment fear     
## # ... with 13,865 more rows
```
]
.pull-right[

```r
get_sentiments("loughran") 
```

```
## # A tibble: 3,850 x 2
##    word         sentiment
##    &lt;chr&gt;        &lt;chr&gt;    
##  1 abandon      negative 
##  2 abandoned    negative 
##  3 abandoning   negative 
##  4 abandonment  negative 
##  5 abandonments negative 
##  6 abandons     negative 
##  7 abdicated    negative 
##  8 abdicates    negative 
##  9 abdicating   negative 
## 10 abdication   negative 
## # ... with 3,840 more rows
```
]

---

class: middle

## Categorizing sentiments

---

## Sentiments in Artists lyrics

.midi[

```r
artist_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 194 x 3
##    sentiment word         n
##    &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;
##  1 positive  like       131
##  2 positive  love        54
##  3 positive  better      50
##  4 negative  trouble     34
##  5 positive  well        24
##  6 negative  miss        23
##  7 positive  right       21
##  8 negative  mad         15
##  9 positive  best        15
## 10 positive  promises    15
## # ... with 184 more rows
```
]

---

class: middle

**Goal:** Find the top 10 most common words with positive and negative sentiments.

---

### Step 1: Top 10 words for each sentiment

.midi[

```r
artist_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) 
```

```
## # A tibble: 22 x 3
## # Groups:   sentiment [2]
##    sentiment word            n
##    &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;
##  1 negative  break          14
##  2 negative  fall           14
##  3 negative  funny           7
##  4 negative  hard            9
##  5 negative  lost            8
##  6 negative  mad            15
##  7 negative  miss           23
##  8 negative  sad             8
##  9 negative  shame           9
## 10 negative  treacherous     7
## # ... with 12 more rows
```
]

---

### Step 2: `ungroup()`

.midi[

```r
artist_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup()
```

```
## # A tibble: 22 x 3
##    sentiment word            n
##    &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;
##  1 negative  break          14
##  2 negative  fall           14
##  3 negative  funny           7
##  4 negative  hard            9
##  5 negative  lost            8
##  6 negative  mad            15
##  7 negative  miss           23
##  8 negative  sad             8
##  9 negative  shame           9
## 10 negative  treacherous     7
## # ... with 12 more rows
```
]

---

### Step 3: Save the result


```r
artist_top10 &lt;- artist_lyrics %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word) %&gt;%
  group_by(sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup()
```

---

### Step 1: Create a bar chart

.midi[

```r
artist_top10 %&gt;%
  ggplot(aes(x = word, y = n, fill = sentiment)) +
  geom_col()
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-32-1.png" width="40%" /&gt;
]

---

### Step 2: Order bars by frequency

.midi[

```r
artist_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col()
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-33-1.png" width="40%" /&gt;
]

---

### Step 3: Facet by sentiment

.midi[

```r
artist_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment)
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-34-1.png" width="40%" /&gt;
]

---

### Step 4: Free the scales!

.midi[

```r
artist_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free")
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-35-1.png" width="40%" /&gt;
]

---

### Step 4: Flip the coordinates

.midi[

```r
artist_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-36-1.png" width="35%" /&gt;
]

---

### Step 5: Clean up labels

.small[

```r
artist_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(title = "Sentiments in Artists lyrics", x = "", y = "")
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-37-1.png" width="40%" /&gt;
]

---

### Step 6: Remove redundant info

.small[

```r
artist_top10 %&gt;%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(title = "Sentiments in Artists lyrics", x = "", y = "") +
  guides(fill = "none") 
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-38-1.png" width="40%" /&gt;
]

---

class: middle

## Scoring sentiments

---

## Assign a sentiment score

.small[

```r
artist_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  left_join(get_sentiments("afinn")) 
```

```
## Joining, by = "word"
## Joining, by = "word"
```

```
## # A tibble: 3,058 x 6
##    artist       album                  track_n track_title          word   value
##    &lt;chr&gt;        &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;                &lt;chr&gt;  &lt;dbl&gt;
##  1 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ walkin    NA
##  2 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ fast      NA
##  3 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ traff~    NA
##  4 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ lights    NA
##  5 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ busy      NA
##  6 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ stree~    NA
##  7 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ busy      NA
##  8 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ lives     NA
##  9 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ touch     NA
## 10 Taylor Swift Red (Taylor's Version)       1 State of Grace (Tay~ chang~    NA
## # ... with 3,048 more rows
```
]

---


```r
artist_lyrics %&gt;%
  anti_join(stop_words) %&gt;%
  left_join(get_sentiments("afinn")) %&gt;%
  dplyr::filter(!is.na(value)) %&gt;%
  group_by(album) %&gt;%
  summarise(total_sentiment = sum(value)) %&gt;%
  arrange(total_sentiment)
```

```
## # A tibble: 1 x 2
##   album                  total_sentiment
##   &lt;chr&gt;                            &lt;dbl&gt;
## 1 Red (Taylor's Version)             138
```

---

![](u2-d11-text-analysis_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;

---



## Text Analysis on a Text - Gathering more data

You can access the full text of many public domain works from Project Gutenberg using the gutenbergr package.

We are pulling from The Last Man by Mary Shelley. Chat with your table: what book do you want to look at? 

```r
library(gutenbergr)
my_mirror &lt;- "http://mirrors.xmission.com/gutenberg/"
full_text &lt;- gutenberg_download(18247, mirror = my_mirror)
```

What book does *your table* want to analyze today? 📖 👯‍♀️ 📖

.footnote[
https://docs.ropensci.org/gutenbergr/
]

---

## Time to tidy your text!


```r
tidy_book &lt;- full_text %&gt;%
  mutate(line = row_number()) %&gt;%
* unnest_tokens(word, text)

glimpse(tidy_book)
```

```
## Rows: 176,606
## Columns: 3
## $ gutenberg_id &lt;int&gt; 18247, 18247, 18247, 18247, 18247, 18247, 18247, 18247, 1~
## $ line         &lt;int&gt; 1, 1, 1, 3, 3, 3, 3, 5, 6, 6, 7, 10, 12, 12, 13, 14, 14, ~
## $ word         &lt;chr&gt; "the", "last", "man", "by", "mary", "wollstonecraft", "sh~
```

---

## What are the most common words?

What do you predict will happen if we run the following code? 🤔


```r
tidy_book %&gt;%
  count(word, sort = TRUE)
```

---

## What are the most common words?

What do you predict will happen if we run the following code? 🤔


```r
tidy_book %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 12,559 x 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 the   11494
##  2 of     7152
##  3 and    6283
##  4 to     5178
##  5 i      2957
##  6 in     2816
##  7 a      2635
##  8 her    2319
##  9 was    2296
## 10 that   1894
## # ... with 12,549 more rows
```

---

# Stop words
.pull-left[

```r
get_stopwords(language = "en")
```

```
## # A tibble: 175 x 2
##    word      lexicon 
##    &lt;chr&gt;     &lt;chr&gt;   
##  1 i         snowball
##  2 me        snowball
##  3 my        snowball
##  4 myself    snowball
##  5 we        snowball
##  6 our       snowball
##  7 ours      snowball
##  8 ourselves snowball
##  9 you       snowball
## 10 your      snowball
## # ... with 165 more rows
```
]
.pull-right[

```r
get_stopwords(source = "smart")
```

```
## # A tibble: 571 x 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           smart  
##  2 a's         smart  
##  3 able        smart  
##  4 about       smart  
##  5 above       smart  
##  6 according   smart  
##  7 accordingly smart  
##  8 across      smart  
##  9 actually    smart  
## 10 after       smart  
## # ... with 561 more rows
```
]


---

## What are the most common words?

SOLVE

```
anti_join(get_stopwords(source = "smart")) %&gt;%
```
```
tidy_book %&gt;%
```
```
count(word, sort = TRUE) %&gt;%
```
```
geom_col() +
```
```
slice_max(n, n = 20) %&gt;%
```
```
ggplot(aes(n, fct_reorder(word, n))) + 
```

---

## What are the most common words?


```r
tidy_book %&gt;%
  anti_join(get_stopwords(source = "smart")) %&gt;%
  count(word, sort = TRUE) %&gt;%
  slice_max(n, n = 20) %&gt;%
* ggplot(aes(n, fct_reorder(word, n))) +
  geom_col()
```


---

class: middle, center


```
## Joining, by = "word"
```

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-49-1.png" width="90%" /&gt;

---

class: center, middle

# SENTIMENT ANALYSIS 😄 😢 😠

---

## Sentiment lexicons


```r
get_sentiments("afinn")
```

```
## # A tibble: 2,477 x 2
##    word       value
##    &lt;chr&gt;      &lt;dbl&gt;
##  1 abandon       -2
##  2 abandoned     -2
##  3 abandons      -2
##  4 abducted      -2
##  5 abduction     -2
##  6 abductions    -2
##  7 abhor         -3
##  8 abhorred      -3
##  9 abhorrent     -3
## 10 abhors        -3
## # ... with 2,467 more rows
```

---

## Implementing sentiment analysis


```r
tidy_book %&gt;%
* inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 2 x 2
##   sentiment     n
##   &lt;chr&gt;     &lt;int&gt;
## 1 negative   8809
## 2 positive   6876
```

---

# Question

What kind of join is appropriate for sentiment analysis?

- `anti_join()`
- `full_join()`
- `outer_join()`
- `inner_join()`

---

## Implementing sentiment analysis

What do you predict will happen if we run the following code? 🤔


```r
tidy_book %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%            
* count(sentiment, word, sort = TRUE)
```

---

## Implementing sentiment analysis

What do you predict will happen if we run the following code? 🤔


```r
tidy_book %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%            
* count(sentiment, word, sort = TRUE)
```

```
## Joining, by = "word"
```

```
## # A tibble: 2,240 x 3
##   sentiment word       n
##   &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;
## 1 positive  love     272
## 2 negative  death    218
## 3 positive  like     183
## 4 negative  fear     138
## 5 negative  plague   128
## 6 negative  lost     116
## 7 negative  dead     107
## # ... with 2,233 more rows
```

---

## Implementing sentiment analysis


```r
tidy_book %&gt;%
  inner_join(get_sentiments("bing")) %&gt;%
  count(sentiment, word, sort = TRUE) %&gt;%
  group_by(sentiment) %&gt;%
  slice_max(n, n = 10) %&gt;%
  ungroup %&gt;%
  ggplot(aes(n,
*            fct_reorder(word, n),
             fill = sentiment)) +
  geom_col() +
  facet_wrap(~ sentiment, scales = "free") 
```

---

class: middle


```
## Joining, by = "word"
```

![](u2-d11-text-analysis_files/figure-html/unnamed-chunk-55-1.png)&lt;!-- --&gt;

---

## Let's bring in some new packages


```r
library(widyr)
library(tidygraph)
library(ggraph)
```

---

## What is a document about?

- Term frequency
- Inverse document frequency

`$$idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$`

### tf-idf is about comparing **documents** within a **collection**.

---

## Understanding tf-idf

Make a collection (*corpus*) for yourself! 💅

Choose two books!
Default is The Last Man and Frankenstein both by Marry Shelley


```r
library(gutenbergr)
full_collection &lt;- gutenberg_download(c(18247, 42324),
        meta_fields = "title",
        mirror = my_mirror)
```

---

## **Understanding tf-idf**

Make a collection (*corpus*) for yourself! 💅


```r
full_collection
```

```
## # A tibble: 24,169 x 3
##    gutenberg_id text                             title       
##           &lt;int&gt; &lt;chr&gt;                            &lt;chr&gt;       
##  1        18247 "The Last Man"                   The Last Man
##  2        18247 ""                               The Last Man
##  3        18247 "by Mary Wollstonecraft Shelley" The Last Man
##  4        18247 ""                               The Last Man
##  5        18247 "LONDON:"                        The Last Man
##  6        18247 "HENRY COLBURN."                 The Last Man
##  7        18247 "1826."                          The Last Man
##  8        18247 ""                               The Last Man
##  9        18247 ""                               The Last Man
## 10        18247 "Contents"                       The Last Man
## # ... with 24,159 more rows
```

---

## Counting word frequencies in your collection


```r
book_words &lt;- full_collection %&gt;%
*   unnest_tokens(word, text) %&gt;%
    count(title, word, sort = TRUE)
```

Think: What do the columns of `book_words` tell us?

---

## book_words

```r
book_words[1:15,]
```

```
## # A tibble: 15 x 3
##    title                                   word      n
##    &lt;chr&gt;                                   &lt;chr&gt; &lt;int&gt;
##  1 The Last Man                            the   11494
##  2 The Last Man                            of     7152
##  3 The Last Man                            and    6283
##  4 The Last Man                            to     5178
##  5 Frankenstein; Or, The Modern Prometheus the    4407
##  6 Frankenstein; Or, The Modern Prometheus and    3058
##  7 The Last Man                            i      2957
##  8 Frankenstein; Or, The Modern Prometheus i      2931
##  9 The Last Man                            in     2816
## 10 Frankenstein; Or, The Modern Prometheus of     2806
## 11 The Last Man                            a      2635
## 12 The Last Man                            her    2319
## 13 The Last Man                            was    2296
## 14 Frankenstein; Or, The Modern Prometheus to     2184
## 15 The Last Man                            that   1894
```

---

## Calculating tf-idf


```r
book_tfidf &lt;- book_words %&gt;%
*   bind_tf_idf(word, title, n)
```

---

## Calculating tf-idf

That's... super exciting???


```r
book_tfidf
```

```
## # A tibble: 19,879 x 6
##    title                                   word      n     tf   idf tf_idf
##    &lt;chr&gt;                                   &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 The Last Man                            the   11494 0.0651     0      0
##  2 The Last Man                            of     7152 0.0405     0      0
##  3 The Last Man                            and    6283 0.0356     0      0
##  4 The Last Man                            to     5178 0.0293     0      0
##  5 Frankenstein; Or, The Modern Prometheus the    4407 0.0563     0      0
##  6 Frankenstein; Or, The Modern Prometheus and    3058 0.0391     0      0
##  7 The Last Man                            i      2957 0.0167     0      0
##  8 Frankenstein; Or, The Modern Prometheus i      2931 0.0374     0      0
##  9 The Last Man                            in     2816 0.0159     0      0
## 10 Frankenstein; Or, The Modern Prometheus of     2806 0.0358     0      0
## # ... with 19,869 more rows
```


---

## Calculating tf-idf

What do you predict will happen if we run the following code? 🤔


```r
book_tfidf %&gt;%
    arrange(-tf_idf)
```

---

## Calculating tf-idf

What do you predict will happen if we run the following code? 🤔


```r
book_tfidf %&gt;%
    arrange(-tf_idf)
```

```
## # A tibble: 19,879 x 6
##   title                                   word          n       tf   idf  tf_idf
##   &lt;chr&gt;                                   &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 The Last Man                            raymond     340 0.00193  0.693 1.33e-3
## 2 The Last Man                            adrian      285 0.00161  0.693 1.12e-3
## 3 The Last Man                            idris       230 0.00130  0.693 9.03e-4
## 4 The Last Man                            perdita     212 0.00120  0.693 8.32e-4
## 5 Frankenstein; Or, The Modern Prometheus elizabeth    88 0.00112  0.693 7.79e-4
## 6 Frankenstein; Or, The Modern Prometheus clerval      59 0.000754 0.693 5.22e-4
## 7 The Last Man                            plague      128 0.000725 0.693 5.02e-4
## 8 Frankenstein; Or, The Modern Prometheus justine      54 0.000690 0.693 4.78e-4
## # ... with 19,871 more rows
```

---

## Calculating tf-idf

SOLVE

```
group_by(title) %&gt;%
```
```
book_tfidf %&gt;%
```
```
slice_max(tf_idf, n = 10) %&gt;%
```
```
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
```
```
facet_wrap(~title, scales = "free")
```
```
geom_col(show.legend = FALSE) +
```
---

## Calculating tf-idf

&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-64-1.png" width="45%" /&gt;

---


```r
book_tfidf %&gt;%
    group_by(title) %&gt;%
    slice_max(tf_idf, n = 10) %&gt;%
    ggplot(aes(tf_idf,
               fct_reorder(word, tf_idf), 
               fill = title)) +
    geom_col(alpha = 0.9, show.legend = FALSE) +
    facet_wrap(~title, scales = "free") +
    scale_x_continuous(expand = c(0,0)) +
    labs(y = NULL, x = "tf-idf")
```

---

class: middle

# N-grams... and beyond! 🚀

---
## N-grams... and beyond! 🚀
Back to just The Last Man (or book of your tables choice)

```r
full_text &lt;- gutenberg_download(18247, mirror = my_mirror)

tidy_ngram &lt;- full_text %&gt;%
*   unnest_tokens(bigram, text, token = "ngrams", n = 2) %&gt;%
    dplyr::filter(!is.na(bigram))
```

---

## N-grams... and beyond! 🚀


```r
tidy_ngram
```

```
## # A tibble: 161,503 x 2
##    gutenberg_id bigram                
##           &lt;int&gt; &lt;chr&gt;                 
##  1        18247 the last              
##  2        18247 last man              
##  3        18247 by mary               
##  4        18247 mary wollstonecraft   
##  5        18247 wollstonecraft shelley
##  6        18247 henry colburn         
##  7        18247 vol i                 
##  8        18247 chapter i             
##  9        18247 chapter ii            
## 10        18247 chapter iii           
## # ... with 161,493 more rows
```


---

## N-grams... and beyond! 🚀


```r
tidy_ngram %&gt;%
    count(bigram, sort = TRUE)
```

```
## # A tibble: 86,556 x 2
##    bigram       n
##    &lt;chr&gt;    &lt;int&gt;
##  1 of the    1424
##  2 in the     676
##  3 to the     648
##  4 and the    443
##  5 on the     402
##  6 from the   325
##  7 of her     282
##  8 by the     277
##  9 of his     267
## 10 it was     266
## # ... with 86,546 more rows
```

---


## Class Question

Can we use an `anti_join()` right away to remove stop words?

- Yes! ✅
- No ☹️

---

## N-grams... and beyond! 🚀


```r
bigram_counts &lt;- tidy_ngram %&gt;%
*   separate(bigram, c("word1", "word2"), sep = " ") %&gt;%
    dplyr::filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %&gt;%
    count(word1, word2, sort = TRUE)
```

---

## N-grams... and beyond! 🚀


```r
bigram_counts
```

```
## # A tibble: 15,647 x 3
##    word1   word2         n
##    &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;
##  1 lord    raymond      36
##  2 fellow  creatures    13
##  3 country people       12
##  4 lord    protector    12
##  5 human   race         11
##  6 windsor castle       11
##  7 chapter iv            8
##  8 ill     fated         8
##  9 poor    girl          8
## 10 heart   beat          7
## # ... with 15,637 more rows
```

---

## What can you do with n-grams?

- tf-idf of n-grams

--

- network analysis

--

- negation

---

&lt;img src="img/austen-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
&lt;img src="img/slider.gif" width="90%" style="display: block; margin: auto;" /&gt;

### [She Giggles, He Gallops](https://pudding.cool/2017/08/screen-direction/)

---

&lt;img src="img/change_overall-1.svg" width="80%" style="display: block; margin: auto;" /&gt;

---

## Network analysis


```r
bigram_graph &lt;- bigram_counts %&gt;%
    dplyr::filter(n &gt; 5) %&gt;%
*   as_tbl_graph()
```

---

## **Network analysis**


```r
bigram_graph
```

```
## # A tbl_graph: 35 nodes and 22 edges
## #
## # A rooted forest with 13 trees
## #
## # Node Data: 35 x 1 (active)
##   name   
##   &lt;chr&gt;  
## 1 lord   
## 2 fellow 
## 3 country
## 4 human  
## 5 windsor
## 6 chapter
## # ... with 29 more rows
## #
## # Edge Data: 22 x 3
##    from    to     n
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1     1    16    36
## 2     2    17    13
## 3     3    18    12
## # ... with 19 more rows
```

---

## Network analysis


```r
bigram_graph %&gt;%
    ggraph(layout = "kk") +
*   geom_edge_link(aes(edge_alpha = n)) +
*   geom_node_text(aes(label = name)) +
    theme_graph() 
```

---

## Network analysis


```r
bigram_graph %&gt;%
    ggraph(layout = "kk") +
*   geom_edge_link(aes(edge_alpha = n),
                   show.legend = FALSE, 
                   arrow = arrow(length = unit(1.5, 'mm')), 
                   start_cap = circle(3, 'mm'),
                   end_cap = circle(3, 'mm')) +
*   geom_node_text(aes(label = name)) +
    theme_graph()
```

---


&lt;img src="u2-d11-text-analysis_files/figure-html/unnamed-chunk-78-1.png" width="50%" /&gt;

---

## Acknowledgements

- Julia Silge: https://github.com/juliasilge/tidytext-tutorial
- Julia Silge and David Robinson: https://www.tidytextmining.com/
- Josiah Parry: https://github.com/JosiahParry/genius
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
