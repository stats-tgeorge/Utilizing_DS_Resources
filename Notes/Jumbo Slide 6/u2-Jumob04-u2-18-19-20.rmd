---
title: "Web scraping"
subtitle: "<br><br> Data Science in a Box"
author: "[datasciencebox.org](https://datasciencebox.org/)    \n Modified by Tyler George"
output:
  xaringan::moon_reader:
    css: ["../xaringan-themer.css", "../slides.css"]
    lib_dir: libs
    anchor_sections: FALSE
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---

```{r child = "../setup.Rmd"}
```

```{r packages, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(DT)
```

class: middle

# Scraping the web

---

## Scraping the web: what? why?

- Increasing amount of data is available on the web
--

- These data are provided in an unstructured format: you can always copy&paste, but it's time-consuming and prone to errors

--
- Web scraping is the process of extracting this information automatically and transform it into a structured dataset

--
- Two different scenarios:
    - Screen scraping: extract data from source code of website, with html parser (easy) or regular expression matching (less easy).
    - Web APIs (application programming interface): website offers a set of structured http requests that return JSON or XML files.

---


# Web Scraping with rvest

---

## Hypertext Markup Language

- Most of the data on the web is still largely available as HTML 
- It is structured (hierarchical / tree based), but it''s often not available in a form useful for analysis (flat / tidy).

```html
<html>
  <head>
    <title>This is a title</title>
  </head>
  <body>
    <p align="center">Hello world!</p>
  </body>
</html>
```

---

## rvest

.pull-left[
- The **rvest** package makes basic processing and manipulation of HTML data straight forward
- It's designed to work with pipelines built with `%>%`
]
.pull-right[
```{r echo=FALSE,out.width=230,fig.align="right"}
knitr::include_graphics("img/rvest.png")
```
]

---

## Core rvest functions

- `read_html`   - Read HTML data from a url or character string
- `html_node `  - Select a specified node from HTML document
- `html_nodes`  - Select specified nodes from HTML document
- `html_table`  - Parse an HTML table into a data frame
- `html_text`   - Extract tag pairs' content
- `html_name`   - Extract tags' names
- `html_attrs`  - Extract all of each tag's attributes
- `html_attr`   - Extract tags' attribute value by name

---

## SelectorGadget

.pull-left-narrow[
- Open source tool that eases CSS selector generation and discovery
- Easiest to use with the [Chrome Extension](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb) 
- Find out more on the [SelectorGadget vignette](https://rvest.tidyverse.org/articles/rvest.html)
]
.pull-right-wide[
```{r echo=FALSE, out.width="75%"}
knitr::include_graphics("img/selector-gadget/selector-gadget.png")
```
]

---

## Using the SelectorGadget

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("img/selector-gadget/selector-gadget.gif")
```

---

```{r echo=FALSE, out.width="95%"}
knitr::include_graphics("img/selector-gadget/selector-gadget-1.png")
```

---

```{r echo=FALSE, out.width="95%"}
knitr::include_graphics("img/selector-gadget/selector-gadget-2.png")
```

---

```{r echo=FALSE, out.width="95%"}
knitr::include_graphics("img/selector-gadget/selector-gadget-3.png")
```

---

```{r echo=FALSE, out.width="95%"}
knitr::include_graphics("img/selector-gadget/selector-gadget-4.png")
```

---

```{r echo=FALSE, out.width="95%"}
knitr::include_graphics("img/selector-gadget/selector-gadget-5.png")
```

---

## Using the SelectorGadget

Through this process of selection and rejection, SelectorGadget helps you come up with the appropriate CSS selector for your needs

```{r echo=FALSE, out.width="65%"}
knitr::include_graphics("img/selector-gadget/selector-gadget.gif")
```

---

## Top 250 movies on IMDB

Take a look at the source code, look for the tag `table` tag (control +u in chrome):
<br>
http://www.imdb.com/chart/top

.pull-left[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/imdb-top-250.png")
```
]
.pull-right[
```{r echo=FALSE, out.width="94%"}
knitr::include_graphics("img/imdb-top-250-source.png")
```
]


---

## First check if you're allowed!

```{r warning=FALSE}
library(robotstxt)
paths_allowed("http://www.imdb.com")
```

vs. e.g.

```{r warning=FALSE}
paths_allowed("http://www.facebook.com")
```

---

## Plan

```{r echo=FALSE, out.width="90%"}
knitr::include_graphics("img/plan.png")
```

---

## Plan

1. Read the whole page

2. Scrape movie titles and save as `titles` 

3. Scrape years movies were made in and save as `years`

4. Scrape IMDB ratings and save as `ratings`

5. Create a data frame called `imdb_top_250` with variables `title`, `year`, and `rating`

---

# Step 1. Read the whole page

---

## Read the whole page

```{r}
page <- read_html("https://www.imdb.com/chart/top/")
page
```

---

## A webpage in R

- Result is a list with 2 elements

```{r}
typeof(page)
```

--

- that we need to convert to something more familiar, like a data frame....

```{r}
class(page)
```

---

# Step 2. Scrape movie titles and save as `titles` 

---

## Scrape movie titles

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/titles.png")
```

---

## Scrape the nodes

.pull-left[
```{r output.lines=17}
page %>%
  html_nodes(".titleColumn a")
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/titles.png")
```
]

---

## Extract the text from the nodes

.pull-left[
```{r output.lines=16}
page %>%
  html_nodes(".titleColumn a") %>%
  html_text()
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/titles.png")
```
]

---

## Save as `titles`

.pull-left[
```{r output.lines=14}
titles <- page %>%
  html_nodes(".titleColumn a") %>%
  html_text()

titles
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/titles.png")
```
]

---

# Step 3. Scrape year movies were made and save as `years`

---

## Scrape years movies were made in

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/years.png")
```

---

## Scrape the nodes

.pull-left[
```{r output.lines=17}
page %>%
  html_nodes(".secondaryInfo")
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/years.png")
```
]

---

## Extract the text from the nodes

.pull-left[
```{r output.lines=16}
page %>%
  html_nodes(".secondaryInfo") %>%
  html_text()
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/years.png")
```
]

---

## Clean up the text

We need to go from `"(1994)"` to `1994`:

- Remove `(` and `)`: string manipulation

- Convert to numeric: `as.numeric()`

---

## stringr

.pull-left-wide[
- **stringr** provides a cohesive set of functions designed to make working with strings as easy as possible
- Functions in stringr start with `str_*()`, e.g.
  - `str_remove()` to remove a pattern from a string
  ```{r}
   str_remove(string = "jello", pattern = "el")
  ```
  - `str_replace()` to replace a pattern with another
  ```{r}
  str_replace(string = "jello", pattern = "j", replacement = "h")
  ```
]
.pull-right-narrow[
```{r echo=FALSE, fig.align="left", out.width="100%"}
knitr::include_graphics("img/stringr.png")
```
]

---

## Clean up the text

```{r output.lines=15}
page %>%
  html_nodes(".secondaryInfo") %>%
  html_text() %>%
  str_remove("\\(") # remove (
```

---

## Clean up the text

```{r output.lines=14}
page %>%
  html_nodes(".secondaryInfo") %>%
  html_text() %>%
  str_remove("\\(") %>% # remove (
  str_remove("\\)") # remove )
```

---

## Convert to numeric

```{r output.lines=13}
page %>%
  html_nodes(".secondaryInfo") %>%
  html_text() %>%
  str_remove("\\(") %>% # remove (
  str_remove("\\)") %>% # remove )
  as.numeric()
```

---

## Save as `years`

.pull-left[
```{r output.lines=11}
years <- page %>%
  html_nodes(".secondaryInfo") %>%
  html_text() %>%
  str_remove("\\(") %>% # remove (
  str_remove("\\)") %>% # remove )
  as.numeric()

years
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/years.png")
```
]

---

# Step 4. Scrape IMDB ratings and save as `ratings`

---

## Scrape IMDB ratings

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/ratings.png")
```

---

## Scrape the nodes

.pull-left[
```{r output.lines=17}
page %>%
  html_nodes("strong")
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/ratings.png")
```
]

---

## Extract the text from the nodes

.pull-left[
```{r output.lines=16}
page %>%
  html_nodes("strong") %>%
  html_text()
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/ratings.png")
```
]

---

## Convert to numeric

.pull-left[
```{r output.lines=15}
page %>%
  html_nodes("strong") %>%
  html_text() %>%
  as.numeric()
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/ratings.png")
```
]

---

## Save as `ratings`

.pull-left[
```{r output.lines=12}
ratings <- page %>%
  html_nodes("strong") %>%
  html_text() %>%
  as.numeric()

ratings
```
]
.pull-right[
```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/ratings.png")
```
]

---

# Step 5. Create a data frame called `imdb_top_250`

---

## Create a data frame: `imdb_top_250`

```{r}
imdb_top_250 <- tibble(
  title = titles, 
  year = years, 
  rating = ratings
  )

imdb_top_250
```

---

```{r echo=FALSE}
options(htmltools.preserve.raw = FALSE)
imdb_top_250 %>% datatable(options(list(dom = "p", pageLength = 8)), height = 400)
```

---

## Clean up / enhance

May or may not be a lot of work depending on how messy the data are

- See if you like what you got:

```{r}
glimpse(imdb_top_250)
```

- Add a variable for rank
```{r}
imdb_top_250 <- imdb_top_250 %>%
  mutate(rank = 1:nrow(imdb_top_250)) %>%
  relocate(rank)
```

---

```{r echo=FALSE}
imdb_top_250 %>%
  print(n = 20)
```

---


# What next?

---

.question[
Which years have the most movies on the list?
]

--

```{r}
imdb_top_250 %>% 
  count(year, sort = TRUE)
```

---

.question[
Which 1995 movies made the list?
]

--

```{r}
imdb_top_250 %>% 
  filter(year == 1995) %>%
  print(n = 8)
```

---

.question[
Visualize the average yearly rating for movies that made it on the top 250 list over time.
]

--

.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "yearly-avg", echo = FALSE, out.width = "58%"}
```
]
.panel[.panel-name[Code]

```{r yearly-avg, fig.show = "hide"}
imdb_top_250 %>% 
  group_by(year) %>%
  summarise(avg_score = mean(rating)) %>%
  ggplot(aes(y = avg_score, x = year)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Year", y = "Average score")
```
]
]


---

## "Can you?" vs "Should you?"

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("img/ok-cupid-1.png")
```

.footnote[.small[
Source: Brian Resnick, [Researchers just released profile data on 70,000 OkCupid users without permission](https://www.vox.com/2016/5/12/11666116/70000-okcupid-users-data-release), Vox.
]]

---

## "Can you?" vs "Should you?"

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/ok-cupid-2.png")
```

---

# Challenges

---

## Unreliable formatting at the source

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/unreliable-formatting.png")
```

---

## Data broken into many pages

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("img/many-pages.png")
```

---

# Workflow

---

## Screen scraping vs. APIs

Two different scenarios for web scraping:

- Screen scraping: extract data from source code of website, with html parser (easy) or regular expression matching (less easy)

- Web APIs (application programming interface): website offers a set of structured http requests that return JSON or XML files

---

## A new R workflow

- When working in an R Markdown document, your analysis is re-run each time you knit

- If web scraping in an R Markdown document, you'd be re-scraping the data each time you knit, which is undesirable (and not *nice*)!

- An alternative workflow: 
  - Use an R script to save your code 
  - Saving interim data scraped using the code in the script as CSV or RDS files
  - Use the saved data in your analysis in your R Markdown document



